{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Building a Neural Network - Simple FCN ( Colored Images)\nCIFAR-10 dataset\n1. The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class.\n2. There are 50000 training images and 10000 test images.\n\nKnow more about data[Click Here](https://www.cs.toronto.edu/~kriz/cifar.html)\n\n-----------------------\n# Summary of All Steps \nExplanation:\n* Loading Data: We load the CIFAR-10 dataset, which is pre-split into training and testing sets.\n* Normalization: Pixel values are divided by 255 to scale them between 0 and 1, helping the neural network converge faster.\n* Model Definition: We build a Sequential model with an input layer, a flatten layer to convert image data into a 1D vector, two hidden dense layers, and an output layer with 10 neurons (one for each class).\n* Model Compilation: We use the Adam optimizer, sparse categorical cross-entropy as the loss function (since we have integer labels), and track accuracy.\n* Model Training: The model is trained for 20 epochs on the training data with validation on the test data after each epoch.\n* Evaluation: The model is evaluated on the test set to calculate accuracy and loss.\n* Prediction and Visualization: The model predicts the classes for the first 5 images in the test set. We display these images along with their predicted and actual labels.","metadata":{}},{"cell_type":"markdown","source":"## loading packages and libraries","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers , models\nimport matplotlib.pyplot as plt ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-02T07:24:46.880633Z","iopub.execute_input":"2024-10-02T07:24:46.881256Z","iopub.status.idle":"2024-10-02T07:24:59.954143Z","shell.execute_reply.started":"2024-10-02T07:24:46.881219Z","shell.execute_reply":"2024-10-02T07:24:59.953121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## loading Data (already splitted)\n**Load CIFAR-10 dataset, which is already split into train and test sets**","metadata":{}},{"cell_type":"code","source":"(x_train,y_train), (x_test,y_test)=tf.keras.datasets.cifar10.load_data()","metadata":{"execution":{"iopub.status.busy":"2024-10-02T07:26:16.580400Z","iopub.execute_input":"2024-10-02T07:26:16.581349Z","iopub.status.idle":"2024-10-02T07:26:30.671587Z","shell.execute_reply.started":"2024-10-02T07:26:16.581275Z","shell.execute_reply":"2024-10-02T07:26:30.670628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Normalizing\n\n* Normalize the pixel values of the images to be between 0 and 1 (original range is 0-255)\n* This helps the model converge faster by keeping input values in a smaller range","metadata":{}},{"cell_type":"code","source":"x_train=x_train/255.0\nx_test=x_test/255.0","metadata":{"execution":{"iopub.status.busy":"2024-10-02T07:27:04.248615Z","iopub.execute_input":"2024-10-02T07:27:04.249421Z","iopub.status.idle":"2024-10-02T07:27:04.888526Z","shell.execute_reply.started":"2024-10-02T07:27:04.249377Z","shell.execute_reply":"2024-10-02T07:27:04.887275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## defining the model\nDefine a simple Artificial Neural Network (ANN) model using Sequential API\n 1. Input layer: We specify the input shape explicitly as (32, 32, 3), where 32x32 are image dimensions and 3 refers to the RGB color channels\n 2. Flatten: Converts 2D image input into 1D for the dense layers\n 3. Dense Layers: Two hidden layers with 512 and 256 neurons respectively, using ReLU activation\n 4. Output Layer: 10 neurons corresponding to the 10 classes in CIFAR-10, using softmax activation for multi-class classification","metadata":{}},{"cell_type":"code","source":"model=models.Sequential([\n     # Add an Input layer\n    layers.Input(shape=(32, 32, 3)),\n\n    # Flatten the input\n    layers.Flatten(),\n\n      # Hidden layers\n    layers.Dense(512,activation='relu'),\n    layers.Dense(256,activation='relu'),\n    \n    \n    # Output layer with 10 classes (for CIFAR-10)\n    layers.Dense(10,activation='softmax')\n    \n\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## model compilation\n\n*  Using 'adam' optimizer for gradient descent and 'sparse_categorical_crossentropy' for loss since the labels are integer encoded\n* We track accuracy as a performance metric\n","metadata":{}},{"cell_type":"code","source":"model.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-10-02T07:36:21.279571Z","iopub.execute_input":"2024-10-02T07:36:21.280004Z","iopub.status.idle":"2024-10-02T07:36:21.295876Z","shell.execute_reply.started":"2024-10-02T07:36:21.279964Z","shell.execute_reply":"2024-10-02T07:36:21.294938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training\n* Train the model on the training data for 20 epochs\n* We also validate it on the test data after each epoch","metadata":{}},{"cell_type":"code","source":"model.fit(x_train,y_train,epochs=20,validation_data=(x_test,y_test))","metadata":{"execution":{"iopub.status.busy":"2024-10-02T07:36:32.001207Z","iopub.execute_input":"2024-10-02T07:36:32.001644Z","iopub.status.idle":"2024-10-02T07:42:43.265928Z","shell.execute_reply.started":"2024-10-02T07:36:32.001604Z","shell.execute_reply":"2024-10-02T07:42:43.264657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation\n* Evaluate the model on the test data\n* This will give us the accuracy and loss on the test set","metadata":{}},{"cell_type":"code","source":"test_loss,test_acc=model.evaluate(x_test,y_test,verbose=2)\nprint(f'\\nTest accuracy: {test_acc}')","metadata":{"execution":{"iopub.status.busy":"2024-10-02T07:43:02.843572Z","iopub.execute_input":"2024-10-02T07:43:02.844484Z","iopub.status.idle":"2024-10-02T07:43:04.102255Z","shell.execute_reply.started":"2024-10-02T07:43:02.844436Z","shell.execute_reply":"2024-10-02T07:43:04.101172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predictions\n* Predictions on the first 5 test images\n* Predict returns the probabilities for each class, so we need to use np.argmax() to get the class with the highest probability","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n\npredictions = model.predict(x_test)  # Predictions will be probabilities for each class\n\nfor i in range(5):  # Show the first 5 test images\n    plt.figure()\n    plt.imshow(x_test[i])  # Display the image\n    \n    predicted_label = np.argmax(predictions[i])  # Predicted class\n    true_label = y_test[i]  # Actual class label\n    \n    plt.title(f'Predicted: {predicted_label}, True: {true_label}')  # Show predicted vs true labels\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T07:46:34.785470Z","iopub.execute_input":"2024-10-02T07:46:34.785952Z","iopub.status.idle":"2024-10-02T07:46:38.037849Z","shell.execute_reply.started":"2024-10-02T07:46:34.785910Z","shell.execute_reply":"2024-10-02T07:46:38.035753Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imporving the Model\n\n* currently as above there are several ways through which we can improve the neural networks's prediction\n1. Switch to a Convolutional Neural Network (CNN)\n2. Increase Model Complexity ( add more layers and neurons)\n3. Data Augmentation\n4. Use Batch Normalization and Dropout\n5. Use a Convolutional Neural Network (CNN)\n6. Use a Learning Rate Scheduler\n7. Try a Pretrained Model\n8. Tune Hyperparameters\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}